{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Computer Vision**"
      ],
      "metadata": {
        "id": "Y0Ag7VkxmAEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YOLO (You Only Look Once)**\n",
        "It is an advanced deep learning model used to object detection in videos and real timefeed. Its the fifth version of the original YOLO model. It is open sourced and available on github.\n",
        "\n",
        "* **Single Neural Network:** YOLOv5 is based onN CNN to predict multiple bounding boxes and classes.\n",
        "* **Image Processing:** It will take the input and convert it into a fixed size frame(640X640) pixels. It will further divide the frame into multiple grids of smaller size.\n",
        "* **Boundary Box Prediction and class prediction:** Boundary box prediction refcers to grid evaluation to decide the start and end of a class. Class prediction means predicting what class that object belongs to.\n",
        "* **Non-max suppression:** To reduce the redundancy of drawing boxes or boundaries, YOLOv5 users a technique called non-max suppresion. It will eliminate the overlap between multiple boundaries."
      ],
      "metadata": {
        "id": "8A8dFMORrs-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n"
      ],
      "metadata": {
        "id": "QSOssWm4rxOp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de7JLrffdVgj",
        "outputId": "f53834a7-9472-4b04-8b01-724cb216e5e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 16575, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 16575 (delta 28), reused 37 (delta 18), pack-reused 16522\u001b[K\n",
            "Receiving objects: 100% (16575/16575), 15.03 MiB | 14.05 MiB/s, done.\n",
            "Resolving deltas: 100% (11387/11387), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ-UlhgRdlm9",
        "outputId": "17ade971-cbe4-4089-ebf7-b69f40871e2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qr requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q3G9axad1kS",
        "outputId": "10586c2e-0dcc-4713-ef50-3984fbc32602"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.1/755.1 kB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "from utils.general import non_max_suppression, check_file"
      ],
      "metadata": {
        "id": "IfQ_9p_Rd7ON"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "9-2ERU_he1d2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name='yolov5s.pt'):\n",
        "  model_path = os.path.join('weights', model_name)\n",
        "  if not os.path.isfile(model_path):\n",
        "    url = f'https://github.com/ultralytics/yolov5/releases/download/v6.0/{model_name}'\n",
        "    !wget {url} -O {model_path}\n",
        "  model = DetectMultiBackend(model_path,device = device)\n",
        "  return model\n",
        "model = load_model()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fwiu5i9hfKQQ",
        "outputId": "ef187885-f4aa-4718-b35c-76a7f4aa1509"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights/yolov5s.pt: No such file or directory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to weights/yolov5s.pt...\n",
            "100%|██████████| 14.1M/14.1M [00:00<00:00, 360MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq_04roWiXkP",
        "outputId": "0cbbca46-a94b-4f90-b980-99fde6715c28"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_frame(frame, model, size=640):\n",
        "  orig_h , orig_w = frame.shape[:2]\n",
        "  frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "  frame_resized = cv2.resize(frame_rgb, (size , size))\n",
        "  frame_transposed = np.transpose(frame_resized , (2,0,1))\n",
        "  frame_norm = frame_transposed / 255.0\n",
        "  frame_tensor = torch.from_numpy(frame_norm).float().unsqueeze(0).to(device)\n",
        "  pred = model(frame_tensor)\n",
        "  pred = non_max_suppression(pred,0.25,0.45,classes=None)\n",
        "  frame_output = frame_rgb.copy()\n",
        "  if pred[0] is not None and len(pred[0]):\n",
        "    det = pred[0]\n",
        "    scale_factors = torch.tensor([orig_w/size,orig_h / size,orig_w / size,orig_h / size],device=device)\n",
        "    det[:, :4] = det[:, :4] * scale_factors\n",
        "    for *xyxy, conf , cls in det:\n",
        "      x1, y1, x2 , y2 = map(int, xyxy)\n",
        "      cv2.rectangle(frame_output, (x1,y1), (x2 , y2), (255,0,0), 2)\n",
        "      cv2.putText(frame_output, f'{model.names[int(cls)]},{conf:.2f}', (x1,y1 -10), cv2.FONT_HERSHEY_SIMPLEX,0.9,(255,0,0), 2)\n",
        "    return cv2.cvtColor(frame_output, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WGPgMaSOjklM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(video_path, output_path, model):\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  vid_write = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'),20, (int(cap.get(3)), int(cap.get(4))))\n",
        "  while cap.isOpened():\n",
        "    ret , frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    processed_frame = process_frame(frame , model)\n",
        "    vid_write.write(processed_frame)\n",
        "  cap.release()\n",
        "  vid_write.release()\n",
        "  print(f\"Processing Completed Succesfully! The saved file is in {output_path}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5b5Ni1Reqi8o"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path='/content/video1.mp4'\n",
        "output_path='/content/ProcessedVideo1.mp4'\n",
        "process_video(video_path,output_path,model)"
      ],
      "metadata": {
        "id": "BFG7T5JktcPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path='/content/video2.mp4'\n",
        "output_path='/content/ProcessedVideo2.mp4'\n",
        "process_video(video_path,output_path,model)"
      ],
      "metadata": {
        "id": "SMV8r7y269nI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}